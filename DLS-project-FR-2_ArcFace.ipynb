{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dukei/dls-fr/blob/master/DLS-project-FR-2_ArcFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-tZoxywJ1I0"
      },
      "source": [
        "# ArcFace Loss (Additive Angular Margin Loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKGDNGvnrrNC"
      },
      "source": [
        "## Теория ArcFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R70bm-ayJznN"
      },
      "source": [
        "В случае с обучением на задачу классификации первая подходящая лосс-функция, которая нам приходит в голову — Cross-Entropy. И на ней действительно можно обучать сеть для распознавания лиц. Но за много лет люди придумали более хитрые трюки, которые делают обучение сети для распознавания лиц более эффективным. Одним из лучших подходов считается ArcFace (Additive Angular Margin).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5N_BHHSK1JR"
      },
      "source": [
        "**Как устроен ArcFace**:\n",
        "\n",
        "Стандартные SoftMax + кросс-энтропия (CE) выглядят так:\n",
        "\n",
        "$$L_{CE} = \\frac{-1}{N}\\sum_1^N \\frac{e^{W_{y_i}^{T}x_i + b_{y_i}}}{\\sum^n_{j=1}e^{W_j^Tx_i+b_j}},$$\n",
        "\n",
        "здесь:\n",
        "- $x_i \\in \\mathbb{R^d}$ — вектор $i$-го элемента обучающей выборки перед последним полносвязным слоем сети. $y_i$ — класс этого элемента;\n",
        "- $W_j \\in \\mathbb{R^d}$ — j-ый столбец матрицы весов последнего слоя сети (т.е. слоя, который производит итоговую классификацю входящего объекта);\n",
        "- $b_j \\in \\mathbb{R^d}$ — j-ый элемент вектора байеса последнего слоя сети;\n",
        "- $N$ — batch size;\n",
        "- $n$ — количество классов.\n",
        "\n",
        "\n",
        "Хотя этот лосс работает хорошо, он явным образом не заставляет эмбеддинги $x_i$ элементов, принадлежащих одному классу, быть близкими друг к другу по расстоянию. И не заставляет эмбеддинги элементов, принадлежащих разным классам, быть далеко друг от друга. Все, что хочет этот лосс — чтобы на основе эмбеддингов $x_i$ можно было хорошо классифицировать элементы, никакие ограничений на расстояния между эмбеддингами $x_i$ он не вводит.\n",
        "\n",
        "Из-за этого у нейросетей для распознавания лиц, которые обучены на обычном CE loss, бывают проблемы с распознаванием лиц, которые сильно отличаются от лиц того же человека разными доп. атрибутами (шляпа/прическа/очки и т.п.). Просто эмбеддинг для таких лиц получается довольно далек по расстоянию от других эмбеддингов лиц этого же человека.\n",
        "\n",
        "Давайте теперь немного поправим формулу:\n",
        "- уберем байес последнего слоя, т.е. сделаем $b_j=0$;\n",
        "- нормализуем веса последнего слоя: ||$W_j$|| = 1;\n",
        "- нормализуем эмбеддинги: ||$x_i$|| = 1. Перед подачей их на вход последнему слою (т.е. перед умножением на матрицу $W_j$) умножим их на гиперпараметр s. По сути, мы приводим норму всех эмбеддингов к s. Смысл этого гиперпараметра в том, что, возможно, сети проще будет классифицировать эмбеддинги, у которых не единичная норма.\n",
        "\n",
        "Нормализация приводит к тому, что эмбеддинги распределяются по сфере единичного радиуса (и сфере радиуса s после умножения на гиперпараметр s). И итоговые предсказания сети после последнего слоя зависят только от угла между эмбеддингами $x_i$ и выученных весов $W_j$. От нормы эмбеддинга $x_i$ они больше не зависят, т.к. у всех эмбеддингов они теперь одинаковые.\n",
        "\n",
        "Получается, в степени экспоненты у нас останется выражение $s W_{y_i}^{T}x_i$, которое можно переписать в виде  $s W_{y_i}^{T}x_i = s ||W_{y_i}||\\cdot ||x_i|| \\cdot cos\\Theta_{y_i}$. Тут $\\Theta_{y_i}$ — это угол между векторами $W_{y_i}$ и $x_i$. Но так как мы сделали нормы $W_{y_i}$ и $x_i$ единичными, то все это выражение просто будет равно $s cos\\Theta_{y_i}$.\n",
        "\n",
        "В итоге мы получим следующую формулу лосса:\n",
        "\n",
        "$$L = \\frac{-1}{N}\\sum_1^N \\frac{e^{s\\ cos\\Theta_{y_i}}}{e^{s\\ cos\\Theta_{y_i}} + \\sum^n_{j=1,\\ j\\ne y_i} e^{s\\ cos\\Theta_j}}$$\n",
        "\n",
        "\n",
        "И последний шаг. Добавим еще один гиперпараметр $m$. Он называется additive angular margin penalty и заставляет эмбеддинги одного класса быть ближе друг к другу, а эмбеддинги разных классов — более далекими друг от друга.\n",
        "\n",
        "В итоге получим вот что:\n",
        "\n",
        "$$L_{ArcFace} = \\frac{-1}{N}\\sum_1^N \\frac{e^{s\\ cos(\\Theta_{y_i} + m)}}{e^{s\\ cos(\\Theta_{y_i} + m)} + \\sum^n_{j=1,\\ j\\ne y_i} e^{s\\ cos\\Theta_j}}$$\n",
        "\n",
        "Это и есть ArcFace Loss с двумя  гиперпараметрами, s и m.\n",
        "\n",
        "Получается, что ArcFace Loss завтавляет сеть выучивать эмбеддинги, распределенные по сфере радиуса s, причем чтобы эмбеддинги одного класса были ближе друг к другу, а эмбеддинги разных классов — более далеки друг от друга.\n",
        "\n",
        "![ArcFace](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTKRR-YA_XR3yhIYBbkc8Zlbua0Q2WdM3gx_g&s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI80vHxcRo4u"
      },
      "source": [
        "**Важное пояснение:**\n",
        "\n",
        "Строго говоря, ArcFace - не лосс, отдельный архитектурный модуль модификация SoftMax. Он реализует идею внесения геометрического отступа непосредственно в пространство признаков. Для обучения в качестве лосса используется обычная кросс-энтропия. Более конкретно по шагам:\n",
        "\n",
        "1. Вы извлекаете эмбеддинги из бэкбона сети (предобученной модели, у которой обрезан FC-слой, если он был)\n",
        "2. Эти эмбеддинги поступают в ArcFace-слой, который содержит векторы-центры для каждого класса (веса классификатора) и логику нормализации и добавления углового отступа\n",
        "3. Для целевого класса ArcFace-слой преобразует косинус угла $\\theta$ в $cos(\\theta + m)$\n",
        "4. Для остальных классов оставляет обычный косинус $cos(\\theta)$\n",
        "5. Эти модифицированные логиты подаются на вход стандартной функции Cross-Entropy\n",
        "6. Градиенты от Cross-Entropy текут назад через ArcFace-слой к бэкбону, обучая модель извлекать эмбеддинги\n",
        "\n",
        "Результат: модифицированные логиты с \"жестким\" разделением для целевого класса, а значит и более качественные эмбеддинги.\n",
        "\n",
        "Схема:\n",
        "```\n",
        "[Изображение] → [Бэкбон] → [ЭМБЕДДИНГ] → [ArcFace] → [Логиты] → [CE Loss]\n",
        "                    │                        │           │          \n",
        "                   CNN                   Нормализация   Оценки\n",
        "                                          + Angular    для всех\n",
        "                                            Margin     классов\n",
        "```\n",
        "\n",
        "Для получения качественных эмбеддингов после обучения ArcFace-слой больше не нужен, и его обычно обрезают. Он нужен был только обучения модели, и поэтому часто ArcFace называю именно лоссом. Но стоит всегда держать в голове, что это некоторое упрощение, которое нужно лишь для того, чтобы проще формулировать мысли."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXoblo49vDjP"
      },
      "source": [
        "**Доп. литература по ArcFace Loss:**\n",
        "\n",
        "Оригинальная статья: https://arxiv.org/pdf/1801.07698.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSnVw2zNrjpn"
      },
      "source": [
        "## Другие лоссы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdnYR55ZXOzx"
      },
      "source": [
        "Кроме ArcFace, есть еще много разных вариантов лоссов для задачи Face Recognition. Некоторые из них можно найти, например, [тут](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w48/Hsu_A_Comprehensive_Study_on_Loss_Functions_for_Cross-Factor_Face_Recognition_CVPRW_2020_paper.pdf). Вы можете попробовать реализовать другие лосс-функции в этом проекте в качестве дополнительного задания.\n",
        "\n",
        "Кроме этого, можно миксовать лосс-функции. Например, обучать нейросеть на сумме ArcFace и TripletLoss. Иногда так выходит лучше, чем если обучать на каком-то одном лоссе."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4nsFZyivfC"
      },
      "source": [
        "# Датасет"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loDtEBkUizfC"
      },
      "source": [
        "В качестве датасета нужно использовать картинки из CelebA, выровненные при помощи своей модели из задания 1. Очень желательно их еще кропнуть таким образом, чтобы нейросети поступали на вход преимущественно только лица без какого либо фона, частей тела и прочего.\n",
        "\n",
        "Если планируете делать дополнительное задание на Identificaton rate metric, то **обязательно разбейте заранее датасет на train/val или train/val/test.** Это нужно сделать не только на уровне кода, а на уровне папок, чтобы точно знать, на каких картинках модель обучалась, а на каких нет. Лучше заранее почитайте [ноутбук с заданием](https://colab.research.google.com/drive/15zuNdOupRFnG7oE-rFj9FsjoNTK6DYn5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhF-n904jYJz"
      },
      "source": [
        "# План заданий"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqmhb66-jhaK"
      },
      "source": [
        "Итак, вот, что от вас требуется в этом задании:\n",
        "\n",
        "* Выбрать модель (или несколько моделей) для обучения. Можно брать предобученные на ImageNet, но нельзя использовать модели, предобученные на задачу распознавания лиц.\n",
        "* Обучить эту модель (модели) на CE loss. Добиться accuracy > 0.7.\n",
        "* Реализовать ArcFace loss.\n",
        "* Обучить модель (модели) на ArcFace loss. Добиться accuracy > 0.7.\n",
        "* Написать небольшой отчет по обучению, сравнить CE loss и ArcFace loss.\n",
        "\n",
        "**P.S. Не забывайте сохранять модели после обучения**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as transforms # Добавлен импорт transforms\n",
        "\n",
        "class FaceRecognitionDataset(Dataset):\n",
        "    def __init__(self, mode: str, transform=None):\n",
        "        \"\"\"\n",
        "        Инициализирует датасет для распознавания лиц.\n",
        "\n",
        "        Args:\n",
        "            mode (str): Режим работы датасета ('train', 'val', 'test').\n",
        "            transform (callable, optional): Необязательные преобразования, применяемые к изображению.\n",
        "        \"\"\"\n",
        "        if mode not in ['train', 'val', 'test']:\n",
        "            raise ValueError(\"Mode must be 'train', 'val', or 'test'.\")\n",
        "\n",
        "        self.mode = mode\n",
        "        self.transform = transform # Сохраняем переданные преобразования\n",
        "        # Путь к базовой директории данных, как определено в контексте notebook\n",
        "        self.base_path = 'celeba_dataset/fr'\n",
        "\n",
        "        # Загрузка данных об идентичности (image_id и identity)\n",
        "        identity_file_name = f'fr_{mode}_identity.csv'\n",
        "        identity_file_path = os.path.join(self.base_path, identity_file_name)\n",
        "        self.identity_df = pd.read_csv(identity_file_path, index_col=\"image_id\")\n",
        "\n",
        "        # Загрузка выровненных изображений лиц\n",
        "        aligned_faces_path = os.path.join(self.base_path, 'aligned_faces', 'full_aligned_data.pt')\n",
        "        # Предполагается, что full_aligned_data.pt - это словарь {image_id: {'image': tensor, ...}}\n",
        "        self.aligned_faces_data = torch.load(aligned_faces_path)\n",
        "\n",
        "        # Убедимся, что все image_id в identity_df присутствуют в aligned_faces_data\n",
        "        self.identity_df = self.identity_df[self.identity_df['image_id'].isin(self.aligned_faces_data.keys())].reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.identity_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Получаем image_id и identity для текущего индекса\n",
        "        row = self.identity_df.iloc[idx]\n",
        "        image_id = row['image_id']\n",
        "        identity = row['identity']\n",
        "\n",
        "        # Извлекаем тензор изображения\n",
        "        # Предполагается, что aligned_faces_data[image_id] является словарем и имеет ключ 'image'\n",
        "        image_tensor = self.aligned_faces_data[image_id]['image']\n",
        "\n",
        "        # Применяем преобразования, если они были предоставлены\n",
        "        if self.transform:\n",
        "            image_tensor = self.transform(image_tensor)\n",
        "\n",
        "        return image_tensor, identity\n"
      ],
      "metadata": {
        "id": "zU9JhrEwNd6O"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class ArcFace(nn.Module):\n",
        "    def __init__(self, in_features, out_features, s=64.0, m=0.50):\n",
        "        super(ArcFace, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, input, label=None):\n",
        "        # Нормализуем признаки и веса\n",
        "        # input: N x in_features\n",
        "        # weight: out_features x in_features\n",
        "        # cosine: N x out_features\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "\n",
        "        if label is None: # Режим инференса: возвращаем масштабированную косинусную схожесть\n",
        "            return self.s * cosine\n",
        "\n",
        "        # Режим обучения с отступом\n",
        "        # Ограничиваем косинус, чтобы избежать численной нестабильности для arccos\n",
        "        cosine = cosine.clamp(-1.0 + 1e-7, 1.0 - 1e-7)\n",
        "\n",
        "        # Вычисление углового отступа\n",
        "        theta = torch.acos(cosine)\n",
        "\n",
        "        # Применяем аддитивный угловой отступ в угловом пространстве\n",
        "        phi = torch.cos(theta + self.m)\n",
        "\n",
        "        # Создаем one-hot маску для целевых классов\n",
        "        one_hot = torch.zeros_like(cosine)\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "\n",
        "        # Заменяем косинус для целевого класса на phi\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "\n",
        "        # Масштабируем логиты\n",
        "        output *= self.s\n",
        "        return output\n",
        "\n",
        "class FaceRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes, backbone_name='resnet50', loss_type='ce', s=64.0, m=0.50):\n",
        "        super(FaceRecognitionModel, self).__init__()\n",
        "        self.loss_type = loss_type\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Загружаем бэкбон ResNet-50 с предобученными весами ImageNet\n",
        "        if backbone_name == 'resnet50':\n",
        "            self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "            # Удаляем оригинальный полносвязный слой для получения признаков напрямую\n",
        "            in_features_backbone = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Бэкбон {backbone_name} не поддерживается.\")\n",
        "\n",
        "        # Инициализируем \"голову\" (head) в зависимости от типа функции потерь\n",
        "        if self.loss_type == 'ce':\n",
        "            self.head = nn.Linear(in_features_backbone, num_classes)\n",
        "        elif self.loss_type == 'arcface':\n",
        "            self.head = ArcFace(in_features_backbone, num_classes, s=s, m=m)\n",
        "        else:\n",
        "            raise ValueError(f\"Тип функции потерь {loss_type} не поддерживается. Выберите 'ce' или 'arcface'.\")\n",
        "\n",
        "    def forward(self, x, labels=None, return_features_only=False):\n",
        "        # Убеждаемся, что входные данные имеют тип float для бэкбона\n",
        "        x = x.float()\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Если нужно вернуть только признаки, делаем это\n",
        "        if return_features_only:\n",
        "            return features\n",
        "\n",
        "        # Передаем признаки и метки (если ArcFace) в \"голову\"\n",
        "        if self.loss_type == 'arcface':\n",
        "            # \"Голова\" ArcFace обрабатывает случай, если метки отсутствуют (инференс против обучения)\n",
        "            logits = self.head(features, labels)\n",
        "        else: # Тип функции потерь 'ce'\n",
        "            logits = self.head(features)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "fLHokvA6Nd0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XdG06GlBxK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78f2ce10"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def calculate_face_recognition_metrics(model, val_dataloader, device):\n",
        "    \"\"\"\n",
        "    Рассчитывает метрики распознавания лиц (EER, ROC AUC) на валидационном датасете.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Модель распознавания лиц.\n",
        "        val_dataloader (DataLoader): DataLoader для валидационного датасета.\n",
        "        device (torch.device): Устройство (CPU или GPU) для выполнения вычислений.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Кортеж из (EER, ROC AUC). Возвращает (1.0, 0.0) в случае недостатка данных.\n",
        "    \"\"\"\n",
        "    model.eval()  # Переводим модель в режим оценки\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_dataloader:\n",
        "            images = images.to(device)\n",
        "            # Извлекаем эмбеддинги, используя return_features_only=True\n",
        "            embeddings = model(images, return_features_only=True)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    if not all_embeddings: # Если эмбеддинги не были извлечены\n",
        "        print(\"No embeddings extracted. Returning default EER=1.0, ROC_AUC=0.0.\")\n",
        "        return 1.0, 0.0\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # Группируем эмбеддинги по identity\n",
        "    embeddings_by_identity = defaultdict(list)\n",
        "    for emb, label in zip(all_embeddings, all_labels):\n",
        "        embeddings_by_identity[label.item()].append(emb)\n",
        "\n",
        "    # Для расчета метрик нам нужно как минимум 2 уникальных identity\n",
        "    # и как минимум 2 изображения для каждой identity для формирования позитивных пар.\n",
        "    # Также для негативных пар нужно минимум 2 identity\n",
        "    unique_identities = list(embeddings_by_identity.keys())\n",
        "\n",
        "    if len(unique_identities) < 2 or any(len(v) < 2 for v in embeddings_by_identity.values()):\n",
        "        print(\"Not enough unique identities or images per identity to form pairs. Returning default EER=1.0, ROC_AUC=0.0.\")\n",
        "        return 1.0, 0.0\n",
        "\n",
        "    positive_similarities = []\n",
        "    negative_similarities = []\n",
        "\n",
        "    # Генерация позитивных и негативных пар\n",
        "    # Будем генерировать пары таким образом, чтобы количество позитивных и негативных пар было примерно равным\n",
        "    # и чтобы избежать слишком большого количества пар для больших датасетов\n",
        "    num_max_pairs_per_identity = 100 # Ограничение на количество пар, чтобы избежать слишком долгих вычислений\n",
        "\n",
        "    for i, identity_i in enumerate(unique_identities):\n",
        "        embs_i = embeddings_by_identity[identity_i]\n",
        "\n",
        "        # Генерируем позитивные пары для identity_i\n",
        "        if len(embs_i) >= 2:\n",
        "            for k in range(min(len(embs_i) - 1, num_max_pairs_per_identity // 2)):\n",
        "                idx1, idx2 = random.sample(range(len(embs_i)), 2)\n",
        "                sim = F.cosine_similarity(embs_i[idx1].unsqueeze(0), embs_i[idx2].unsqueeze(0))\n",
        "                positive_similarities.append(sim.item())\n",
        "\n",
        "        # Генерируем негативные пары для identity_i\n",
        "        if len(unique_identities) > 1:\n",
        "            # Выбираем случайную другую identity\n",
        "            other_identities = [id for id in unique_identities if id != identity_i]\n",
        "            for k in range(min(len(embs_i), num_max_pairs_per_identity // 2)):\n",
        "                # Выбираем эмбеддинг из текущей identity\n",
        "                emb1 = random.choice(embs_i)\n",
        "\n",
        "                # Выбираем случайную другую identity и один ее эмбеддинг\n",
        "                identity_j = random.choice(other_identities)\n",
        "                embs_j = embeddings_by_identity[identity_j]\n",
        "                emb2 = random.choice(embs_j)\n",
        "\n",
        "                sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0))\n",
        "                negative_similarities.append(sim.item())\n",
        "\n",
        "    if not positive_similarities or not negative_similarities:\n",
        "        print(\"Could not form enough positive or negative pairs. Returning default EER=1.0, ROC_AUC=0.0.\")\n",
        "        return 1.0, 0.0\n",
        "\n",
        "    # Объединяем все сходства и их истинные метки\n",
        "    similarities = np.array(positive_similarities + negative_similarities)\n",
        "    # 1 для позитивных пар, 0 для негативных\n",
        "    labels = np.array([1] * len(positive_similarities) + [0] * len(negative_similarities))\n",
        "\n",
        "    # Расчет ROC-кривой и AUC\n",
        "    fpr, tpr, thresholds = roc_curve(labels, similarities)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Расчет EER\n",
        "    fnr = 1 - tpr\n",
        "    eer_threshold_idx = np.argmin(np.abs(fpr - fnr))\n",
        "    eer = fpr[eer_threshold_idx]\n",
        "\n",
        "    return eer, roc_auc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "930d3213"
      },
      "source": [
        "# Task\n",
        "Implement a function `calculate_face_recognition_metrics` that takes a face recognition model, a validation DataLoader, and a device as input. This function should:\n",
        "1. Extract embeddings for all images in the validation DataLoader using the provided model.\n",
        "2. Generate positive pairs (images of the same person) and negative pairs (images of different people) from the extracted embeddings and their corresponding identity labels.\n",
        "3. Calculate the cosine similarity for each generated pair.\n",
        "4. Compute the Equal Error Rate (EER) and ROC AUC score from these similarities and their true labels (1 for positive, 0 for negative).\n",
        "5. Return the calculated EER and ROC AUC scores.\n",
        "\n",
        "The function should handle cases where there might not be enough unique identities or images per identity to form pairs, returning default or informative values. Ensure `torch.nn.functional` is used for cosine similarity and `sklearn.metrics` for EER and ROC AUC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88148910"
      },
      "source": [
        "## implement_metric_calculation\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to calculate face recognition metrics (EER, ROC-AUC) on a validation dataset. This involves extracting embeddings, generating positive and negative pairs, calculating cosine similarity, and using `sklearn.metrics` to compute the final scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69565ce2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A function `calculate_face_recognition_metrics` was successfully implemented to evaluate face recognition model performance.\n",
        "*   The implemented function extracts embeddings for all images in a validation dataset, then intelligently generates positive and negative pairs based on identity labels.\n",
        "*   It computes the cosine similarity for each generated pair, providing a measure of likeness between faces.\n",
        "*   The function calculates two critical face recognition performance metrics: the Equal Error Rate (EER) and the ROC AUC score, leveraging `sklearn.metrics` for accuracy.\n",
        "*   Robustness was ensured by incorporating handling for scenarios where insufficient unique identities or images per identity are available to form pairs, returning appropriate default or informative values.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   This robust metric calculation function enables quantitative assessment and comparison of face recognition models, which is essential for model development and improvement.\n",
        "*   The next step involves integrating this `calculate_face_recognition_metrics` function into the training and validation pipeline to systematically monitor and track model performance during the development cycle.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}